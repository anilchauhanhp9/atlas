---
title: "Atlas Tutorial Part I"
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
```
## Intro
In the first part of the tutorial you will learn how to install Atlas, init a new project and run it. Everithing is done in the command line.


Do you need some music to work. Have a look at this [spotify playlist](https://open.spotify.com/playlist/1uJJpcPx752ddZXCtU6oRc?si=sTO-ec95TFqxHviin59M0g) for bioinformaticians.


### Setup 

**If you work on Windows don't open this webpage in Microsoft edge, it doesn't diplay all th interactive part correctly**

1. Connect to the server on this link  http://18.198.190.146:8000/
2. Log in with the credentials you receved before.
3. Go to the terminal

![](media/go_to_terminal_jupyter.png)

It is as if you would connect to the server of your institution. 

### Conda


The only dependency for metagenome atlas is the *conda package manager*. It can easily be installed with the miniconda package. 

On the server we have already installed it. You only need to activate it by running:
```{bash, eval=FALSE}
/opt/miniconda3/bin/conda init
exec bash
```

To have access to all up-to-date bioinformatic packages you should add tell conda too look for them in the *conda-forge* and the *bioconda* channel in addition to the default.


```{bash eval=F}
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
```

These two channels are community-driven efforts to make many software packages installable as easy as possible.


Different programs need often different versions of python/R or other packages. It can become a nightmare to install different tools with conflicting dependencies. Conda allows you to encapsulate each software into its own *environment*. 

--> Create a conda environment with to you name, then activate it.
```{bash eval=F}
conda create --yes -n yourname
```


You should see a `(yourname)` at the beginning of the bash line.

### Install atlas

Now let's install metagenome-atlas.

```{r setupa, echo=FALSE}
question("What is the command to install metagenome-atlas",
  answer("conda install metagenome-atlas",correct = TRUE, message="mamba is a faster alternative to conda, but both work."),
  answer("mamba install metagenome-atlas", correct = TRUE),
  answer("snakemake install metagenome-atlas")
)
```

--> Run the fastest way to install metagenome-atlas.

Run `atlas --help`
Do you get a message? Then atlas is installed correctly.

```{r init1, echo=FALSE}
question("What is the subcommand that you will run to start a new project?",
  answer("atlas download", message="All databases and software is installed on the fly. Download is optional."),
  answer("atlas init",correct = TRUE, message = "Run the command with the '--help' to see what attributes you need"),
  answer("atlas run")

)
```

## Initialisation
### Start a new project


To start a new project you need the path to the fastq files of your metagenome samples (You analyze all samples together). We have provided you with two samples of test data in the folder `/data/test_reads`.

The other parameter you should set is the database directory. This should point to a path where you can store the >100GB databases and software installed automatically by metagenome-atlas. Ideally, this is also a shared location with your colleges. For the Tutorial, we will simply use `/data/databases` folder which already exists.

--> run the init command

```{bash, eval=FALSE}
atlas init --db-dir /data/databases /data/test_reads
```

```{r init, echo=FALSE}
question("What files did atlas create directory?",
  answer("test_reads"),
  answer("databases",message="the database folder actually existed before"),
  answer("config.yaml", correct = TRUE),
  answer("atlas"),
  answer("samples.tsv",correct = TRUE)
)
```



### Configure


--> Have a look at the `samples.tsv` with `cat samples.tsv`. 
Check if the names of the samples are inferred correctly. Samples should be alphanumeric names and cam be dash delimited. The `BinGroup` parameter can be used to activate co-binning. All samples in the same BinGroup are used mapped against each other. However, as it doesn't make sense to use this feature for less than three samples we don't use it and let the default.

Let's check the `config.yaml` file. Open it in the integrated file browser on the jupyter page (The first page you entered wafter the login). It contains many parameters to configure the pipeline. They are explained in the comments or more in detail in the documentation.


```{r config, echo=FALSE}
message="The 'assembly_memory' uses up to 250GB"
question("With the default configuration. How much memory would be used maximally? ",
  answer("250 MB", message="Memory units are in GB"),
  answer(" 60 GB",message= message),
  answer("250 GB", correct=T),
  answer(" 60 MB",message= message)
)
```

If you scrol down to `contaminant_references`, cou can alredy see that the Phix genome is added as contaminant. The Phix is a virus that is frequently used as control for sequencing. Even if you haven't used it in your sequencing there is some chance that some reads might swim around in the sequencer.

Let's add a host genome that should be removed during the decontamination step.
You should find a `human_genome.fasta` in your database folder. 

<!--
First, find out the absolute path to the human genome, then add it to the config file in the section `contaminant_references`.
-->

Adapt the contaminant_references as follows:
```
contaminant_references:
  PhiX: /data/databases/phiX174_virus.fa
  human: /data/databases/human_genome.fasta
```
<!--
Don't just copy the snippet above you need to replace `/path/to/` with the correct absolute path. It's the same for both contaminant references.
-->
Pay attention that there are two spaces at the beginning of the line. Finally, save the file.

## Run atlas

### Dry Run

Before Running the pipeline, which can take more than a day it is always recommended to do a dry-run. This simulates an execution and checks if there are any errors in the config file or elsewhere.

--> Run `atlas run --help` to see how to do a dry-run.
--> Call the run command with the dry-run.

**In case you missed the dry-run parameter the use `CTR+C` to stop the run.**

The dry-run command takes a while and then it shows a list of all the steps that would be executed.

```{r dry, echo=FALSE}
question("How many steps would be executed by atlas?",
  answer("4"),
  answer("145", correct = TRUE),
  answer("174")
)
```

This command runs all the steps in the atlas pipeline for two staples. You can see how it would scale for more samples.

### Profile 

If you run metagenome-atlas on your cluster (or cloud) you should set up a cluster profile, as described in the documentation. For the demo, I made a profile called `Demo`. On a cluster, each step can be automatically submitted to the cluster. In this case, you can run many jobs or steps in parallel. Even when you don't want to submit jobs to the cluster you can run metagenome-atlas on a server with high memory. In this case, atlas can also run many jobs in parallel. The Demo profile limits the memory usage so that one step is executed after the other.


**Important: always run atlas with the `--profile Demo` parameters otherwise the Tutorial server can crash.** 


To understand better what atlas does we, begin to run only the quality control and assembly sub-workflow (QC + assembly). For your project, you don't need to run QC separately. You can directly run all steps with `atlas run all` from the beginning.



--> Run the following command. 

```{bash eval=F}
atlas run assembly --profile Demo 
```


```{r demoass, echo=FALSE}
message="The Demo profile adapts the memory requirement to the system."
question("How much memory are used for any step?",
  answer("250 GB",message=message),
  answer("2 GB", correct = TRUE, message=message),
  answer("60 GB",message=message)
)
```

<!--
### Automatic installation of software

You should see something like:
```{bash, eval=FALSE}
Creating conda environment /usr/local/envs/atlasenv/lib/python3.6/site-packages/atlas/rules/../envs/required_packages.yaml...
Downloading and installing remote packages.
```


At the beginning, atlas installs all the software it needs in conda environments. This can take quite some time but has to be done only once. You can easily run metagenome-atlas in a screen and come back some hours later. But wait -- stay here!  For the tutorial, we have already installed all except one conda environment so the

Once the conda environments are installed atlas will start with the steps of the pipeline. 

-->

## QC + Assembly
### Reports

While the pipeline is runing you can already answer the flowing questions?

The main output file of the qc and the assembly sub-workflow is a reports.

The `reports/QC_report.html` gives a graphical report on the most important numbers.
The dataset used for the Tutorial is a very small one, here you can see the [QC report](https://metagenome-atlas.readthedocs.io/en/latest/_static/QC_report.html) of a bigger run.


```{r qcreport2, echo=FALSE}
message= "Sample F26 has lost many reads during the quality filtering, maybe it would make sense to drop it altogether."
question("In the bigger run, are all samples of good quality? ",
  answer("yes", message=message),
  answer("no",message=message, correct = TRUE)
)
```

Once the assembly sub-workflow is finished, it will again produce a report `reports/assembly_report.html`. You can open [this one](https://metagenome-atlas.readthedocs.io/en/latest/_static/assembly_report.html) until your pipeline has finished.

The assembly report shows different statistics about the length and number of the contigs.  

Download the extended statistics at the bottom of the report, and open the text file with Excel or a text editor.

```{r sizeass, echo=FALSE}
message="A quart of an E. coli genome! "
question("What is the longest contig in any sample?",
  answer("~ 1 Kbp",message=message),
  answer("~ 1 Mbp", correct = TRUE, message=message),
  answer("~ 1 Gbp",message=message)
)
```

Info: there is also the option to use scaffolding with the spades assembler to combine contigs that can be linked by paired-reads.

### QC output

There are also many files produced for each sample. Can you find the quality-controlled reads (.fastq) for sample1?

```{r qcfastq, echo=FALSE}
message="We have paired-reads. The third file contains the reads that lost their mate during the equality control. They are seamlessly integrated in the pipeline."
question("How many qc fastq files are there per sample?",
  answer("1", message = message),
  answer("2", message = message),
  answer("3",correct = TRUE, message = message)
)
```



<!--
Once the QC sub-workflow has finished, check what files are listed as input to the `qc` rule.

```{bash, eval=FALSE}
[Wed Nov 25 11:13:45 2020]
localrule qc:
    input: sample2/sequence_quality_control/finished_QC, sample1/sequence_quality_control/finished_QC, stats/read_counts.tsv, stats/insert_stats.tsv, stats/read_length_stats.tsv, reports/QC_report.html

```
This are the main output files of the QC workflow for all samples.

-> Try to open the `stats/read_counts.tsv` with the integrated text editor. Which sample has more reads?
-->


### Assembly output










Once the assembly workflow is finished try to find the contigs file for sample 2 (.fasta file). Open it.


```{r nsample, echo=FALSE}
question("What is the name of the first contig? ",
  answer("sample_0"),
  answer("sample2_1"),
  answer("sample2_0",correct = TRUE, message='In python we start to count from 0, it is also the longest contig in the file.')
)
```



## Stop and go


You might wonder what happens if the server crashes during the long execution of Atlas.

Run the dry run command for binning

```{bash, eval=F}
atlas run binning --profile Demo
```

-> How many steps would now be executed by atlas? Remember the number
–> run the command without the dry-run.

Wait until one or two steps have finished then. –> press `CTR + C.`

This simulates a system crash. The pipeline should stop and do some cleanup.

Now run again the dry-run command. How many steps would now be executed by atlas?
Do you see, that metagenome-atlas can continue to run the pipeline form the where it stopped? There are even checkpoints during the assembly from which it can continue.




## Binning
### Introduction
Run the binning workflow until the end

```{bash eval=F}
atlas run binning --profile Demo
```


We try to reconstruct genomes from the metagenome.
This is done by grouping together contigs which we think belong to the same genome.
These groups are called **bins**, and if we really think it is a genome then we call it **MAG for metagenome-assembled genome**

![](https://merenlab.org/momics/03-reconstructing-genomes-from-metagenomes.gif){width=15cm}


*Here is a short animation from the a class of Prof. Eren https://merenlab.org/momics/*


By default, we use the automatic binners metabat2 and maxbin2 and then a bin refining with DAS-Tool.
Both binners are based on the sequence composition and the coverage profile in one sample. To get the coverage profile we first need to align the reads to the assembly.

### Bins

Once the pipeline has finished, let's look at the output.

Each binner produces a `cluster_attribution.tsv` which provides the link between contig and bin.
Have a look at the one from sample 2 produced by both binners, by running:
```{bash eval=F}
head sample2/binning/*/cluster_attribution.tsv
```

```{r qbin1, echo=FALSE}
question("In which bin is the longest contig of sample 2? ",
    answer("sample2_metabat_1",correct=TRUE),
    answer("sample2_metabat_2"),
    answer("sample2_maxbin_1",correct=TRUE),
    answer("sample2_maxbin_2"),
    answer("No clue.", message="the longest contig as stated above is 'sample2_0'")
)
```

Maxbin produces also a summary file. Have a look.

```{bash eval=F}
cat sample2/binning/maxbin/sample2.summary
```

```{r qbin2, echo=FALSE}
question("What is the completeness of the worst bin? ",
  answer("< 90%"),
  answer("< 60%",correct=TRUE)
)
```

### Bin refining

The DAS Tool takes the predictions of both binners and tries to find a harmonization with the best result, meaning the highest quality. For example: The bin `sample2_metabat_1` and `sample2_maxbin_1` contain both the longest contig `sample2_0` and many others. The DAS tool chooses the better of the two.  For this, the DAS Tool also estimates the quality of the bins.

Have a look at the plot in: `sample2/binning/DASTool/sample2_DASTool_scores.pdf`

It shows how DAS tool estimates the quality of both biners and its results.

```{r qbin3, echo=FALSE}
question("Which binner produces the better results?",
  answer("metabat",correct=TRUE, message = "This is something we see throughout, e.g. also in the publication of DAS Tool."),
  answer("maxbin"),
  answer("DAS tool",message="DAS Tool is actually not a binner, rather a bin refiner.")
)
```


```{r qbin4, echo=FALSE}
question("Is the quality estimate of DAS Tool as bad as the one from maxbin for the lower quality bins?",
  answer("No, DAS Tool thinks bins are not as bad as maxbin.",correct=TRUE, message = "The reason is they use a different set of marker genes"),
  answer("Yes, DAS Tool estimates bins are also of low quality.", message = "No, the reason is they use a different set of marker genes")
)
```

### Quality assessment
It is not easy to find a set of marker genes for each genome, especially if the genome belongs to unknown species. And different sets yield different results. Usually, the final quality estimates is made with BUSCO or CheckM, which adapt the marker gene set to better estimate the quality.

Unfortunately, this step cannot be run until the end as the genome quality estimation needs more than 100GB of RAM.

But, you can look [here](https://rawcdn.githack.com/metagenome-atlas/Tutorial/31f89f0bdf476ab34b7e52ebd68ef14e20b9c677/media/bin_report_DASTool.html) at the final output of the binning report.

```{r qbin5, echo=FALSE}
question("Was the low quality estimation of maxbin correct at the end? Assuming that the checkM quality estimation in the report is closest to the truth?",
  answer("maxbin quality estimation was way to low",correct=TRUE, message = "The species in this dataset have very small genomes, this may cause the low quality estimation."),
  answer("maxbin quality estimation was ok")
)
```




```{r qbin6, echo=FALSE}
question("How many different species are there in both samples? ",
  answer("2"),
  answer("3",correct=TRUE, message = ),
  answer("5", message = "some genomes from sample 1 and 2 belong to the same species")
)
```

The *genomes* sub-workflow of Atlas combines the binning results from different samples and produces a non-redundant set of MAGs. The workflow also quantifies the genomes in all the samples and annotates them with a better taxonomy.

## End



Because of the computational and time limits we won't run atlas until the end. It won't anyway not be very interesting for this small dataset.  Instead, you will analyze the output of a more interesting project in part 2 of the Tutorial.



You can no go [Part 2 of the Tutorial](https://metagenome-atlas.shinyapps.io/Part2). On the flowing pages are some extra exercises about the atlas gene-catalog workflow.





## Extra

### Gene catalog



```{bash eval=F}
atlas run genecatalog --profile Demo --working-dir workdir
```
